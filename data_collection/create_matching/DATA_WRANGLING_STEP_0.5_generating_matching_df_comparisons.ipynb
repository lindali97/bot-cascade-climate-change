{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d9abf-41c1-44a1-9ce8-662aeb6a3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090e5db-323a-4ab5-b398-df142a2c71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299448a5-e2d9-4f1f-9c01-8d79a7700d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_path = \"./0422_rf_edition/xr_users_classified_0422_wprob.csv\"\n",
    "user_prob_path = \"./0422_rf_edition/xr_users_classified_0704_four_methods_wprob.csv\"\n",
    "tweet_path = \"tweet_table_xr_2019_0621_fixed.csv\"\n",
    "tweet_topic_modelled = \"./NLP/text_classified.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a6e8f-72d8-4ce6-b132-1cd10c183802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54930dc-85f4-4b98-8e6b-e174c346a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_classification_col = \"is_bot_botometer\"\n",
    "suffix = \"_botometer_0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b9609-bc94-47bf-998f-e9ac09439217",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate users w/ bot interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235a3ab-1be4-4609-b782-bfd282db52ac",
   "metadata": {},
   "source": [
    "### Filtering communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e958e9-c2d4-4c2f-ac76-9ef8e83d6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "users = pd.read_csv(user_path, lineterminator='\\n')\n",
    "users_prob = pd.read_csv(user_prob_path, lineterminator='\\n')\n",
    "df = pd.read_csv(tweet_path)\n",
    "tweet_classified = pd.read_csv(tweet_topic_modelled, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d09a29-3c34-4ec7-a598-d1e8533ddaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.merge(users_prob[[\"id\",\"is_bot_xgb\",\"is_bot_xgb_prob\",\"is_bot_botometer_prob\"]], on = \"id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10cd90-0eb3-4447-a7cb-d9de37a9fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out tweet not related to XR like iphone sellers\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)iphone\")]\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)phone\")]\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)max\")]\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)11\")]\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)vr\")]\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)xpel\")]\n",
    "df = df[~df[\"text\"].str.contains(\"(?i)tint\")]\n",
    "#text = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e531f-6ad6-4e40-afd0-d1d10b2006e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy() # keep a copy for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcee9b-2d6e-43bf-a982-6158480fe729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at_dt'] = pd.to_datetime(df.created_at, infer_datetime_format=True)\n",
    "users['created_at_dt'] = pd.to_datetime(users.created_at, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871e80a-d291-48be-a4e0-1bf3213874e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_col = \"is_bot_botometer_prob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790d137-0af3-4655-a447-3a4709fb3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(users[[prob_col,'id']], how = 'left', left_on = 'author_id', right_on = 'id', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29209532-4674-4cb9-99dc-fde8ac9dd444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = [prob_col], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106ca12-8330-4629-99b0-0d4bde7b0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_interactions(prob_col, threshold, df):\n",
    "    df = df[df.referenced_tweets_0_type != \"retweeted\"]\n",
    "    bot_classification_col = \"_\".join(prob_col.split(\"_\")[:-1])\n",
    "    \n",
    "    df = df.merge(tweet_classified, how = 'left', left_on = \"text\", right_on = \"text\", copy = False)\n",
    "\n",
    "    df_comm = df.merge(users[[prob_col, 'id']], how = 'left', left_on = 'in_reply_to_user_id', right_on = 'id', suffixes=('', '_reply'))\n",
    "    #print(df_comm.columns)\n",
    "    df_comm[bot_classification_col] = [1 if i>threshold else 0 for i in df_comm[prob_col]]\n",
    "    df_comm[bot_classification_col+\"_reply\"] = [1 if i>threshold else 0 for i in df_comm[prob_col+\"_reply\"]]\n",
    "    reply_df = df_comm[(df_comm[bot_classification_col+\"_reply\"] == 1) & (df_comm[bot_classification_col] == 0)]\n",
    "    return reply_df, df_comm\n",
    "\n",
    "def generate_matching(reply_df, original_df, threshold, prob_col = \"is_bot_botometer_prob\"):\n",
    "    reply_df_short = reply_df[[\"author_id\", \"conversation_id\", \"created_at\"]]\n",
    "    \n",
    "    bot_classification_col = \"_\".join(prob_col.split(\"_\")[:-1])\n",
    "    original_df[bot_classification_col] = [1 if i>threshold else 0 for i in original_df[prob_col]]\n",
    "    \n",
    "    original_df = original_df[(original_df[bot_classification_col] == 0) & (original_df.referenced_tweets_0_type != \"replied_to\")]\n",
    "    #reply_df_short['created_at_dt'] = pd.to_datetime(reply_df_short.created_at, infer_datetime_format=True)\n",
    "    original_df['created_at_rough'] = [i[:-9] for i in original_df.created_at]\n",
    "    reply_df_short['created_at_rough'] = [i[:-9] for i in reply_df_short.created_at]\n",
    "    matched_records = original_df.merge(reply_df_short, how = 'right', on = \"created_at_rough\", copy = False)\n",
    "    matched_records = matched_records[matched_records.author_id_x != matched_records.author_id_y]\n",
    "    matched_uids = matched_records[[\"author_id_x\", \"author_id_y\", \"created_at_x\",\"created_at_y\"]]\n",
    "    matched_uids.columns = ['matched_user', \"user\", \"interaction_matched\", \"interaction\"]\n",
    "    return matched_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d33e2e-4cd0-4f35-96c7-a483d07a8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [i/100 for i in range(0,90,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09136d-06db-45b3-af71-8312af488719",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_pct = []\n",
    "num_users = []\n",
    "total_user_records = []\n",
    "num_for_matching = []\n",
    "total_for_matching_records = []\n",
    "for threshold in thresholds:\n",
    "    bot_pct.append(sum([1 if i>threshold else 0 for i in users[\"is_bot_botometer_prob\"]])/users.shape[0])\n",
    "    reply_df = filtering_interactions(\"is_bot_botometer_prob\", threshold, df)[0]\n",
    "    matched_df = generate_matching(reply_df, df, threshold)\n",
    "    num_users.append(len(set(reply_df.author_id)))\n",
    "    total_user_records.append(reply_df.shape[0])\n",
    "    num_for_matching.append(len(set(matched_df.matched_user)))\n",
    "    total_for_matching_records.append(matched_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b297a-2357-4830-b9ba-3607d47f93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527e33c-df13-4457-939b-80a213063262",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_pct = [1-i for i in bot_pct]\n",
    "color_map = [\"#fc2720\", \"#1170aa\"]\n",
    "plt.stackplot(thresholds,bot_pct,human_pct, colors = color_map)\n",
    "plt.ylabel('% of bots identified',fontsize = 14)\n",
    "plt.xlabel('Thresholds',fontsize = 14)\n",
    "plt.xlim(0,0.85)\n",
    "plt.ylim(0,1)\n",
    "plt.text(0.1, 0.2, 'Bots', fontsize=16, color='white')\n",
    "plt.text(0.7, 0.9, 'Humans', fontsize=16, color='white')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./botometer_based_svg/bot_pct_by_threshold_all.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993cc47-b6d8-41da-aacb-20a0b0f5ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "fig, axs = plt.subplots(2, 2, figsize = (9,7), sharex = True)\n",
    "\n",
    "# Plot the subfigures\n",
    "axs[0, 0].plot(thresholds,num_users, color = \"#fc7d0b\")\n",
    "axs[0, 0].set_ylabel('Number of users',fontsize = 14)\n",
    "axs[0, 1].plot(thresholds,num_for_matching, color = \"#57606c\")\n",
    "axs[0, 1].set_ylabel('Number of potential matching users',fontsize = 14)\n",
    "axs[1, 0].plot(thresholds,total_user_records, color = \"#fc7d0b\")\n",
    "axs[1, 0].set_ylabel('Sample records',fontsize = 14)\n",
    "\n",
    "axs[1, 1].plot(thresholds,total_for_matching_records, color = \"#57606c\")\n",
    "axs[1, 1].set_ylabel('Potential matching user records',fontsize = 14)\n",
    "fig.text(0.5, 0.001, 'Probability threshold', ha='center',fontsize = 18)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()\n",
    "plt.savefig(\"./botometer_based_svg/num_users_by_threshold_0,5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699cede-9b73-4e8a-afff-4794733e5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_df,df_comm = filtering_interactions(\"is_bot_botometer_prob\", 0.5, df)\n",
    "potential_matched = generate_matching(reply_df, df, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b937e9-86fa-421f-99ef-56dccb0c7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_df.author_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad1b63-cfa8-40a6-8ad1-99207a30285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_df[reply_df.author_id == 2059811]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b686b-1e15-48a0-b0a7-7a9fb64ef36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matched = potential_matched.drop_duplicates(subset = ['matched_user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a2600-6199-4976-a97f-2226f082d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(potential_matched.user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98b022-b8f3-4505-81d1-ae2da4438ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_df.to_csv(f\"./botometer/xr2019_user_reply_bots{suffix}.csv\", index = False)\n",
    "df_comm.to_csv(f'./botometer/df_comm_edges{suffix}.csv', index = False)\n",
    "potential_matched.to_csv(f\"./botometer/xr2019_user_matched_rough{suffix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff95300-e19b-4b37-b497-35476c6e4582",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_post_id = [i if i is not None else j for i,j in zip(reply_df.referenced_tweets_0_id, reply_df.referenced_tweets_1_id)]\n",
    "bot_post_id = [str(int(i)) for i in bot_post_id if str(i) != \"nan\"] # Saving the ids of bot posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f38793-6a52-4577-be04-0848dc92a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./botometer/bot_conversation_id_0.5.txt', 'w') as f:\n",
    "    for line in bot_post_id:\n",
    "        f.write(str(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7972a-ce60-40be-a708-ec2b72ceb1d2",
   "metadata": {},
   "source": [
    "## Creating table for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47e49c-6c42-4327-8f03-f27a4fa40549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 60 day time range before & after interaction for data collection\n",
    "user_time = users.merge(reply_df[[\"author_id\", \"created_at\", \"text\"]], how = 'inner', left_on = \"id\", right_on = \"author_id\", copy = False)\n",
    "user_time = user_time[['author_id','created_at_x', 'created_at_y', 'text']]\n",
    "user_time.columns = ['author_id', 'user_created_at', 'interaction_at', 'text']\n",
    "user_time = user_time.drop_duplicates(subset = ['author_id'])\n",
    "user_time['user_created_at'] = pd.to_datetime(user_time.user_created_at, infer_datetime_format=True)\n",
    "user_time['interaction_at'] = pd.to_datetime(user_time.interaction_at, infer_datetime_format=True)\n",
    "time_window = timedelta(days = 30)\n",
    "user_time['time_start'] = user_time.interaction_at - time_window\n",
    "user_time['time_end'] = user_time.interaction_at + time_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6051cc-a95e-4ade-803c-7b30db32f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_time['time_start_str'] = [i.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for i in user_time.time_start]\n",
    "user_time['time_end_str'] = [i.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for i in user_time.time_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8dc50c-0091-4aac-88ac-cb995f9e5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_time.to_csv(f\"./botometer/matching_user_data_collection_xr2019{suffix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84d0c3-b07f-4d4c-bba4-7aeb08b90a1d",
   "metadata": {},
   "source": [
    "## Filtering by eculid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06df87d-12c2-440b-90f2-bf5420ae745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users[users.created_at != \"0\"]\n",
    "#users.dropna(subset = ['created_at'], inplace = True)\n",
    "#users['created_at_dt'] = pd.to_datetime(users.created_at, format = \"%a %b %d %H:%M:%S +0000 %Y \", errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a1160-6cac-43ed-847d-379d50c63601",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matched = potential_matched.drop_duplicates(subset = ['matched_user'])\n",
    "euclid = pd.DataFrame(columns = ['uid', \"matched_uid\", \"euclid\"])\n",
    "cols = ['statuses_count', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'followers_growth', 'friends_growth', 'favourites_growth',\n",
    "       'listed_growth', 'follower_friend_ratio']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4429c4f-350d-43e8-9743-f95e8aff6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "matched_user = []\n",
    "eculid = []\n",
    "for i in range(potential_matched.shape[0]):\n",
    "    uid = potential_matched.iloc[i].user\n",
    "    matched_uid = potential_matched.iloc[i].matched_user\n",
    "    user_info = users[users.id == uid][cols]\n",
    "    matched_info = users[users.id == matched_uid][cols]\n",
    "    if user_info.shape[0] == 1 and matched_info.shape[0] == 1:\n",
    "        user_info = [np.log(float(i)+1) for i in list(user_info.T.iloc[:,0])]\n",
    "\n",
    "        matched_info = [np.log(float(i) + 1) for i in list(matched_info.T.iloc[:,0])]\n",
    "\n",
    "        X = np.vstack([user_info, matched_info])\n",
    "        euclidean_ = distance.pdist(X)[0]\n",
    "        uids.append(uid)\n",
    "        eculid.append(euclidean_)\n",
    "        matched_user.append(matched_uid)\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398edf22-25a5-4f69-9e66-7fcc7dd27360",
   "metadata": {},
   "outputs": [],
   "source": [
    "eculid_df = pd.DataFrame()\n",
    "eculid_df['uid'] = uids\n",
    "eculid_df['eculid'] = eculid\n",
    "eculid_df['matched_uid'] = matched_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b39be-8d47-4e43-8640-72890de5ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eculid_df.sort_values(by = ['uid', 'eculid'], ascending = [True, False], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1362bca-0b24-4c6d-825a-55251fc9bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_with_euclid = eculid_df.merge(potential_matched, right_on = 'matched_user', left_on = 'matched_uid', how = 'left', copy = False)\n",
    "matched_with_euclid.dropna(subset = ['matched_user'], inplace = True)\n",
    "matched_with_euclid['interaction_matched'] = pd.to_datetime(matched_with_euclid.interaction_matched, infer_datetime_format=True)\n",
    "time_window = timedelta(days = 30)\n",
    "matched_with_euclid['time_start'] = matched_with_euclid.interaction_matched - time_window\n",
    "matched_with_euclid['time_end'] = matched_with_euclid.interaction_matched + time_window\n",
    "matched_with_euclid.to_csv(f\"./botometer/xr2019_user_matched_with_euclid{suffix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d118d8-99a8-4980-87f8-34623791249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first best match\n",
    "firsts = matched_with_euclid.drop_duplicates(subset=['uid'], keep='first')\n",
    "firsts['time_start'] = pd.to_datetime(firsts.time_start, infer_datetime_format=True)\n",
    "firsts['time_end'] = pd.to_datetime(firsts.time_end, infer_datetime_format=True)\n",
    "firsts['time_start_str'] = [i.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for i in seconds.time_start]\n",
    "firsts['time_end_str'] = [i.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for i in seconds.time_end]\n",
    "firsts.to_csv(f\"./xr2019_user_matched_firsts{suffix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa54e07-6f46-4725-b2c4-a3feecf03f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting second best match\n",
    "seconds = matched_with_euclid.groupby('uid').head(2)\n",
    "seconds['uid'] = [str(int(i)) for i in seconds.uid]\n",
    "seconds['matched_uid'] = [str(int(i)) for i in seconds.matched_uid]\n",
    "seconds['matched_user'] = [str(int(i)) for i in seconds.matched_user]\n",
    "seconds['time_start'] = pd.to_datetime(seconds.time_start, infer_datetime_format=True)\n",
    "seconds['time_end'] = pd.to_datetime(seconds.time_end, infer_datetime_format=True)\n",
    "seconds['time_start_str'] = [i.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for i in seconds.time_start]\n",
    "seconds['time_end_str'] = [i.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for i in seconds.time_end]\n",
    "seconds.to_csv(f\"./botometer/xr2019_user_matched_fifths{suffix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500dfac1-d22e-4bc6-a158-f9e3d6224d23",
   "metadata": {},
   "source": [
    "## bot original stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0ab4c-871b-4d68-86ce-b4a051c1c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token(textfile):\n",
    "    try:\n",
    "        with open(textfile, 'r') as file:\n",
    "            auth = file.readlines()\n",
    "            keys = []\n",
    "            for i in auth:\n",
    "                i = str(i).strip()\n",
    "                keys.append(i)\n",
    "        return keys\n",
    "    except EnvironmentError:\n",
    "        print('Error loading access token from file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d9f8a-a4ee-415c-857f-cceb6c4f38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = load_token(\"./botometer/bot_conversation_id_0.5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ff8b7-9c84-4c35-9da6-ebdf37cbbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv(tweet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd5a6d-53be-4d00-83d9-acd9c6637f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [int(i) for i in conversation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b558c8-da75-403b-99b3-7b79154974c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_original_tweet = all_tweets.merge(pd.DataFrame({\"id\":conversation}),on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fafe2-390b-4dbd-81bc-2d08c4c4738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_original_tweet.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d2980-259a-42ad-8761-a2bd2441385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_original_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87fd27-e2e4-4fd1-ab96-2a83418ccd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
